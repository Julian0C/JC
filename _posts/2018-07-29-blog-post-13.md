---
title: 'Chinese Opening Brief Named Entity Recognition with Bidirectional LSTM-CRF Model'
date: 2018-07-29
permalink: /posts/2018/07/blog-post-13/
comments: true
tags:
  - DL
  - NER
  - LSTM

---
## Introduction
Named Entity Recognition (NER) is one of the most important tasks in NLP and is a challenging task requiring large amount of knowledge in the form of feature engineering to achieve high performance. A bi-directional LSTM model, powerful in sequence modeling, can capture infinite amount of contest from both sides of a word and conditional random field (CRF) is a conditional probability model for making ordered data, which combines the characteristics of maximum entropy model and the HMM model. 
According in this, in this article, we applied bidirectional LSTMs and conditional random field on Chinese opening brief dataset with different wording embedding layer approaches to achieve best performance. Our purpose is to classifier company names and human names for further analysis. For one Chinese sentence, each character in this sentence will be assigned to a tag which belongs to the set {O, B-COM, I-COM, B-PER, I-PER}. 
<p float="left">
	<img src="/JC/images/ner.png" width="400" />
</p>

**Figure 1: Architecture of the model**

## Model Design
We use a LSTM-CRF model to implement NER for Chinese opening brief dataset. As shown in Figure 1, the model proposed in this article contains four components：
1)	Word Embedding Layer: map words to vectors of real numbers in a low-dimensional space
2)	Bidirectional LSTM Layer: utilize both past and future input information and extract features automatically
3)	CRF Layer: labels the tag for each character in one sentence
**2.1 Word Embedding Layer**

1)	Initial embedding matrix randomly 
2)	Pretrained word2vec word embedding layer on Chinese opening brief dataset
3)	Pretrained word2vec word embedding layer on Chinese opening brief dataset and Chinese news text dataset 
**2.2 Bidirectional LSTM Layer**

RNNs have been employed to produce promising results on variety natural language tasks and are a family of neural network designed for sequence data. Unfortunately, RNNs tend to be biased towards most recent inputs in the sequence. 
LSTM networks can efficiently make use of past features and further features and incorporate a memory-cell showing great capabilities to capture long-range dependencies. 
**2.3 CRF Layer**

CRF layer labels the tag for each character in each sentence. If we apply SoftMax layer for labeling we might get ungrammatical tag sequences because SoftMax could only label each position independently. CRF layer can use sentence-level tag information and model the transition behavior of each two different tags. 

## Experiment 
**3.1 Data Sets**

We test our three models on Chinese opening brief data set. This dataset contains two types of named entities: person and company. Chinese word segmentation is not included in test set.  

**Table 1. Dataset Overview**

| Dataset| #Sentence| #O| #B-PER|#I-PER|#B-ORG|#I-ORG|
| -------| ------ | ----|-------|------|------|------|
|TrainSet|33080|1785361|48803|89575|15434|195866|
|TestSet|8271|450131|12176|22362|3919|49135|

**3.2 Results**

Table1 presents our comparison of classification report with different word embedding layers. Figure 2 presents the right rate of classification after we combine characters into person names and organization names and compare them with right names from original sentences. We tag sentence as right if all names match, then calculate what is the right rate of each NER results. Since the task is comparatively simple, we observe that random embedding can achieve best result. 

<p float="left">
	<img src="/JC/images/ner2.png" width="400" />
</p>

**Figure 2: Model Results Comparison**

**Table 2. Three embedding approaches’ Right Rate**

|  | Right Rate| 
| -------| ------ |
|Random Embedding|92.29%|
|Pre-trained Embedding(Openning Brief)| 92.25%|
|Pre-trained Embedding(Openning Brief+News Text)| 86.62%|

## Conclusion 
This article presents our neural network model, character-based BILSTM-CRF and can be transferred to any other domains easily. In further, we would like to apply another model to perform relationship extraction to figure out prosecution and defense based on NER results. With NER and relationship extraction, we would be able to transform opening brief into standard format for further study. 




Reference
========

[1].Jinhang Wu, Xiao Hu, Rongsheng Zhao, Feiliang Ren, Minghan Hu, 2017, Clinical Named Entity Recognition via Bi-directional LSTM-CRF Model

[2].Jason P.C. Chiu, Eric Nichols, 2015, Named Entity Recognition with Bidirectional LSTM-CNNs

[3].Zhiheng Huang, Wei Xu, Kai Yu, 2015, Bidirectional LSTM-CRF Models for Sequence Tagging

[4].Chuanhai Dong, Jiajun Zhang, Chengqing Zong, Masanori Hattori, Hui Du, 2016, Character-Based LSTM-CRF with Radical-Level Features for Chinese Named Entity Recognition

------